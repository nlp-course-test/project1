{"cells": [{"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "view-in-github"}, "source": "<a href=\"https://colab.research.google.com/github/nlp-course/materials/blob/tmp_psets/distrib/project1/project1_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "JyMdovPAKAHU"}, "source": "# Project 1: Text classification\n\nIn this homework you will build several varieties of text classifiers using PyTorch.\n\n1. A majority baseline.\n2. A naive Bayes classifer.\n3. A logistic regression classifier.\n4. A multilayer perceptron classifier."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "jKCmwuSH0acV"}, "source": "## Setup"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "ll7qHvRemmTD"}, "outputs": [], "source": "!pip install -qU torchtext"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "kdi-spgB0sEi"}, "outputs": [], "source": "import os\nimport re\nimport copy\nfrom collections import Counter\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torchtext as tt\n\n# Set random seeds\nseed = 1234\ntorch.manual_seed(seed)\n\n# GPU check, make sure to set runtime type to \"GPU\" where available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint (device)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "drbeoB66kJLd"}, "source": "### Load Data\n\nFor this and future project segments, you will be working with the [ATIS (Airline Travel Information System) dataset](https://www.kaggle.com/siddhadev/atis-dataset-from-ms-cntk). This dataset is composed of queries about flights \u2013 their dates, times, locations, airlines, and the like. Over the years, the dataset has been annotated in all kinds of ways, with parts of speech, informational chunks, parse trees, corresponding SQL queries. You'll use various of these annotations in future assignments. For this project segment, however, you'll pursue an easier classification task: **given a query, predict the answer type**.\n\nBelow is an example taken from this dataset:\n\n_Query:_\n\n```\nshow me the afternoon flights from washington to boston\n```\n\n_SQL:_\n\n```\nSELECT DISTINCT flight_1.flight_id FROM flight flight_1 , airport_service airport_service_1 , city city_1 , airport_service airport_service_2 , city city_2 \n   WHERE flight_1.departure_time BETWEEN 1200 AND 1800 \n     AND ( flight_1.from_airport = airport_service_1.airport_code \n           AND airport_service_1.city_code = city_1.city_code \n           AND city_1.city_name = 'WASHINGTON' \n           AND flight_1.to_airport = airport_service_2.airport_code \n           AND airport_service_2.city_code = city_2.city_code \n           AND city_2.city_name = 'BOSTON' )\n```\n\nIn this problem set, we will consider the answer type for a query to be the target field of the corresponding SQL query. For the above example, the answer type would be *flight_id*.\n\nFirst, let's download the dataset."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "zGVWcvlk080Q"}, "outputs": [], "source": "!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/train.nl\n!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/train.sql\n!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/dev.nl\n!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/dev.sql\n!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/test.nl\n!wget -nv -N -P data https://raw.githubusercontent.com/nlp-course/data/master/ATIS/test.sql"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "tPJ6ihGH1Oz8"}, "source": "### Process the data\n\nWe use `torchtext` to process the data. More information on `torchtext` can be found at https://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/.\n\nTo begin, `torchtext` requires that we define a mapping from the raw text data to featurized indices, called a [`Field`](https://torchtext.readthedocs.io/en/latest/data.html#fields). We need one field for processing the question (`TEXT`), and another for processing the label (`LABEL`). These fields make it easy to map back and forth between readable data and lower-level representations like numbers."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "n0XvgL1X6GiK"}, "outputs": [], "source": "TEXT = tt.data.Field(lower=True, # lowercased\n                     sequential=True, # sequential data\n                     include_lengths=False, # do not include lengths\n                     batch_first=True, # batches will be batch_size X max_len\n                     tokenize=tt.data.get_tokenizer(\"basic_english\")) \nLABEL = tt.data.Field(batch_first=True, sequential=False, unk_token=None)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "fTkktPQa13yJ"}, "source": "We provide an interface for loading ATIS data built on top of `torchtext.data.Dataset`. "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "qiZRD-ua1Jfm"}, "outputs": [], "source": "class ATIS(tt.data.Dataset):\n  @staticmethod\n  def sort_key(ex):\n    return len(ex.text)\n\n  def __init__(self, path, text_field, label_field, **kwargs):\n    \"\"\"Creates an ATIS dataset instance given a path and fields.\n    Arguments:\n        path: Path to the data file\n        text_field: The field that will be used for text data.\n        label_field: The field that will be used for label data.\n        Remaining keyword arguments: Passed to the constructor of\n            tt.data.Dataset.\n    \"\"\"\n    fields = [('text', text_field), ('label', label_field)]\n    \n    examples = []\n    # Get text\n    with open(path+'.nl', 'r') as f:\n        for line in f:\n            ex = tt.data.Example()\n            ex.text = text_field.preprocess(line.strip()) \n            examples.append(ex)\n    \n    # Get labels\n    with open(path+'.sql', 'r') as f:\n        for i, line in enumerate(f):\n            label = self._get_label_from_query(line.strip())\n            examples[i].label = label\n            \n    super(ATIS, self).__init__(examples, fields, **kwargs)\n  \n  def _get_label_from_query(self, query):\n    \"\"\"Returns the answer type from `query` by dead reckoning.\n    It's basically the second or third token in the SQL query.\n    \"\"\"    \n    match = re.match(r'\\s*SELECT\\s+(DISTINCT\\s*)?(\\w+\\.)?(?P<label>\\w+)', query)\n    if match:\n        label = match.group('label')\n    else:\n        raise RuntimeError(f'no label in query {query}')\n    return label\n\n  @classmethod\n  def splits(cls, text_field, label_field, path='./',\n              train='train', validation='dev', test='test',\n              **kwargs):\n    \"\"\"Create dataset objects for splits of the ATIS dataset.\n    Arguments:\n        text_field: The field that will be used for the sentence.\n        label_field: The field that will be used for label data.\n        root: The root directory that the dataset's zip archive will be\n            expanded into; therefore the directory in whose trees\n            subdirectory the data files will be stored.\n        train: The filename of the train data. Default: 'train.txt'.\n        validation: The filename of the validation data, or None to not\n            load the validation set. Default: 'dev.txt'.\n        test: The filename of the test data, or None to not load the test\n            set. Default: 'test.txt'.\n        Remaining keyword arguments: Passed to the splits method of\n            Dataset.\n    \"\"\"\n\n    train_data = None if train is None else cls(\n        os.path.join(path, train), text_field, label_field, **kwargs)\n    val_data = None if validation is None else cls(\n        os.path.join(path, validation), text_field, label_field, **kwargs)\n    test_data = None if test is None else cls(\n        os.path.join(path, test), text_field, label_field, **kwargs)\n    return tuple(d for d in (train_data, val_data, test_data)\n                  if d is not None)\n"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "bGx_0Dk_d7fc"}, "source": "We load the data splits, and build the vocabularies from the training data."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "aetkuF_1d1nb"}, "outputs": [], "source": "# Make splits for data\ntrain_data, val_data, test_data = ATIS.splits(TEXT, LABEL, path='./data/')\n\n# Build vocabulary for data fields\nMIN_FREQ = 3 # words appearing less than 3 times are treated as 'unknown'\nTEXT.build_vocab(train_data, min_freq=MIN_FREQ)\nLABEL.build_vocab(train_data)\n\n# Compute size of vocabulary\nvocab_size = len(TEXT.vocab.itos)\nnum_labels = len(LABEL.vocab.itos)\nprint(f\"Size of vocab: {vocab_size}\")\nprint(f\"Number of labels: {num_labels}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "5HCgGp4ACIvL"}, "source": "Note that we mapped words appearing less than 3 times to a special _unknown_ token `<unk>` for two reasons: first, due to the scarcity of such rare words in training data, we might not be able to learn generalizable conclusions about them; second, introducing an unknown token allows us to deal with out-of-vocabulary words in the test data as well: we just map those words to `<unk>`."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Tr5Omf6yBTsI"}, "outputs": [], "source": "unk_token = TEXT.unk_token\nprint (f\"Unknown token: {unk_token}\")\nunk_index = TEXT.vocab.stoi[unk_token]\nprint (f\"Unknown word id: {unk_index}\")\n\n# UNK example\nexample_unk_token = 'IAmAnUnknownWordForSure'\nprint (f\"An unknown token: {example_unk_token}\")\nprint (f\"Mapped back to word id: {TEXT.vocab.stoi[example_unk_token]}\")\nprint (f\"Mapped to <unk>: {TEXT.vocab.stoi[example_unk_token] == unk_index}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "go2q9-vd6RO7"}, "source": "To load data in batches, we use `data.BucketIterator`. This enables us to iterate over the dataset under a given `BATCH_SIZE` which specifies how many instances we want at a time."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "QBVa2Krb5IcY"}, "outputs": [], "source": "BATCH_SIZE = 32\ntrain_iter = tt.data.BucketIterator(train_data, batch_size=BATCH_SIZE, device=device)\nval_iter = tt.data.BucketIterator(val_data, batch_size=BATCH_SIZE, device=device)\ntest_iter = tt.data.Iterator(test_data, batch_size=BATCH_SIZE, sort=False, device=device)"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "UBpXYa4h5g5I"}, "source": "Let's look at a single batch from one of these iterators."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "r6QHKuQ75bjd"}, "outputs": [], "source": "batch = next(iter(train_iter))\ntext = batch.text\nprint (f\"Size of text batch: {text.size()}\")\nprint (f\"Third sentence in batch: {text[2]}\")\nprint (f\"Converted back to string: {' '.join([TEXT.vocab.itos[i] for i in text[2]])}\")\n\nlabel = batch.label\nprint (f\"Size of label batch: {label.size()}\")\nprint (f\"Third label in batch: {label[2]}\")\nprint (f\"Converted back to string: {LABEL.vocab.itos[label[2].item()]}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "5iL-spuDoLDt"}, "source": "You might notice some padding tokens `<pad>` when we convert word ids back to string, or equivalently, padding ids `1` in the corresponding tensor. The reason why we need such padding is because the sentences in a batch might be of different lengths, and to save them in a 2D tensor for parallel processing, sentences that are shorter than the longest sentence need to be padded with some placeholder values. Note that during training we need to make sure that the paddings do not affect the final results."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "ifE_-aPo81x7"}, "outputs": [], "source": "padding_token = TEXT.pad_token\nprint (f\"Padding token: {padding_token}\")\n\npadding_id = TEXT.vocab.stoi[padding_token]\nprint (f\"Padding word id: {padding_id}\")"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "H6rFmJcj-rgl"}, "source": "Alternatively, we can also directly iterate over the individual examples in `train_data`, `val_data` and `test_data`. Here the returned values are the raw sentences and labels instead of word ids, and you might need to explicitly deal with the unknown words, unlike using bucket iterators which automatically map unknown words to an unknown word id."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "Q2GGBhTF-5p0"}, "outputs": [], "source": "for example in train_iter.dataset: # train_iter.dataset is just train_data\n  print (\"Sentence:\", example.text)\n  print (\"label:\", example.label)\n  break"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "CxDMbHJG9Qpg"}, "source": "## Part 1: Establish a majority baseline\n\nA simple baseline for classification tasks is to always predict the most common class. \n\n**Implement the majority baseline and compute test accuracy using the starter code below.** Note that for this baseline, and the naive Bayes classifier later, we don't need to use the validation set since we don't tune any hyper-parameters."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "zm5Bp7OR-bA6"}, "outputs": [], "source": "#TODO\ndef majority_baseline_accuracy(train_iter, test_iter):\n  '''Returns the most common label in the training set, and the accuracy of\n  the majority baseline on the test set.\n  '''\n  \"your code here\"\n  return most_common_label, test_accuracy"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "vOC7A_34v1zB"}, "outputs": [], "source": "# Call the method to establish a baseline\nmost_common_label, test_accuracy = majority_baseline_accuracy(train_iter, test_iter)\n\nprint(f'Most common label: {most_common_label}\\n'\n      f'Test accuracy:     {test_accuracy:.3f}')"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "dNbq_QvG_XGY"}, "source": "## Part 2: Implement a Naive Bayes classifier\n\n### Review of Naive Bayes\n\n$$\n   \\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n   \\newcommand{\\Prob}{{\\Pr}}\n   \\newcommand{\\given}{\\,|\\,}\n   \\newcommand{\\vect}[1]{\\mathbf{#1}}\n   \\newcommand{\\cnt}[1]{\\sharp(#1)}\n$$\nRecall from lab 3 that the Naive Bayes classification method classifies a text $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ as  the class $c_i$ given by the following maximization:\n$$\n\\argmax{i} \\Prob(c_i \\given \\vect{x}) \\approx \\argmax{i} \\Prob(c_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given c_i)\n$$\nor equivalently (since taking the log is monotonic)\n$$ \\begin{align*}\n\\argmax{i} \\Prob(c_i \\given \\vect{x}) &= \\argmax{i} \\log\\Prob(c_i \\given \\vect{x}) \\\\\n&\\approx \\argmax{i} \\left(\\log\\Prob(c_i) + \\sum_{j=1}^m \\log\\Prob(x_j \\given c_i)\\right)\n\\end{align*}$$\n\nAll we need, then, to apply the Naive Bayes classification method is values for the various log probabilities $\\log\\Prob(c_i)$ and $\\log\\Prob(x_j \\given c_i)$ for each feature $x_j$ and each class $c_i$.\n\nWe can estimate the prior probabilities $\\Prob(c_i)$ by examining the empirical probability in the training set. That is, we estimate \n\n$$ \\Prob(c_i) \\approx \\frac{\\cnt{c_i}}{N} $$\n\nWe can estimate the likelihood probabilities $\\Prob(x_j \\given c_i)$ similarly by examining the empirical probability in the training set. That is, we estimate \n\n$$ \\Prob(x_j \\given c_i) \\approx \\frac{\\cnt{x_j, c_i}}{\\sum_{j'} \\cnt{x_{j'}, c_i}} $$\n\nTo handle cases in which the count $\\cnt{x_j, c_i}$ is zero, we can adjust this estimate using add-$\\delta$ smoothing:\n\n$$ \\Prob(x_j \\given c_i) \\approx \\frac{\\cnt{x_j, c_i} + \\delta}{\\sum_{j'} \\cnt{x_{j'}, c_i} + \\delta \\cdot V} $$\n\nwhere $V$ is the total vocabulary size."}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "bJDEXnaESogl"}, "source": "$$ \\newcommand{\\Prob}{{\\Pr}}\n   \\newcommand{\\given}{\\,|\\,}\n$$\n### Implementation\n \nFor the implementation, we ask you to implement a Python class `NaiveBayes` that will have (at least) the following three methods:\n\n1. `__init__`: An initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples.\n\n2. `train`: A method that takes a training data iterator and estimates all of the log probabilities $\\log\\Prob(c_i)$ and $\\log\\Prob(x_j \\given c_i)$ as described above. Perform add-$\\delta$ smoothing with $\\delta=1$. These probabilities will be used by the `evaluate` method to evaluate a test dataset for accuracy, so you'll want to store these probabilities in some data structures in objects of the class.\n\n3. `evaluate`: A method that takes a test data iterator and evaluates the accuracy of the trained model on the test set.\n\nYou should expect to achieve about an **86% accuracy** on the ATIS task."}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "v9s0ha3aS9PW"}, "outputs": [], "source": "#TODO\nclass NaiveBayes():\n  def __init__ (self, text, label):\n    self.text = text\n    self.label = label\n    self.padding_id = text.vocab.stoi[text.pad_token]\n    self.V = len(text.vocab.itos) # vocabulary size\n    self.N = len(label.vocab.itos) # the number of classes\n    #TODO: Implement this method.\n    \"your code here\"\n    \n  def train(self, iterator):\n    \"\"\"Populates tables of log probabilities for training dataset `iterator`.\"\"\"\n    #TODO: Implement this method.\n    \"your code here\"\n    \n  def evaluate(self, iterator):\n    \"\"\"Returns the model's performance on a given dataset `iterator`.\"\"\"\n    #TODO: Implement this method.\n    \"your code here\"\n    return accuracy"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "WOO52qZMv1zS"}, "outputs": [], "source": "# Instantiate and train classifier\nnb_classifier = NaiveBayes(TEXT, LABEL)\nnb_classifier.train(train_iter)\n\n# Evaluate model performance\nprint(f'Training accuracy: {nb_classifier.evaluate(train_iter):.3f}\\n'\n      f'Test accuracy:     {nb_classifier.evaluate(test_iter):.3f}')"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "-pH4ph3uHnvD"}, "source": "## Part 3: Logistic regression\n\nIn this part, you'll complete a PyTorch implementation of a logistic regression (equivalently, a single layer perceptron) classifier.\n\n### Review of logistic regression\n\nIn logistic regression, we assign a weight $w^c_x$ to each word type $x\\in\\mathcal{V}$ and each label $c$. Then for a text $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ we can model $\\Prob (c \\given \\mathbf{x})$ as\n\n$$ \\Prob(c \\given \\mathbf{x}) = \\sigma\\left(\\sum_{i=1}^m w^c_{x_i}\\right), $$\n\nwhere $\\sigma$ is the softmax function, a generalization of the sigmoid function from lab 4:\n\n$$ \\Prob(c \\given \\mathbf{x}) = \\frac{\\exp\\left(\\sum_{i=1}^m w^c_{x_i}\\right)}{\\sum_{c'} \\exp\\left(\\sum_{i=1}^m w^{c'}_{x_i}\\right)}\n$$\n\nHere, we're treating the types of the individual words in the text to be the features, and \"looking up\" the weights. Since each word is treated separately, the order of words doesn't matter. Consequently, we can use the bag of words representation introduced in lab 1. Recall that the bag-of-words representation of a text is just the frequency distribution over the vocabulary, which we can notate $bow(\\mathbf{x})$. Given a vocabulary of word types $\\mathbf{v} = \\langle v_1, v_2, \\ldots, v_V \\rangle$, the representation of a sentence $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ is a vector of size $V$, where $$bow(\\mathbf{x})_j = \\sum_{i=1}^m 1[x_i = v_j]$$\n\n(We write $1[x_i = v_j]$ to indicate 1 if $x_i = v_j$ and 0 otherwise.)\n\nThen, we can rewrite logistic regression as: \n\n$$ \\begin{align*}\n\\Prob(c \\given \\mathbf{x})\n    &= \\sigma\\left(\\sum_{i=1}^m w^c_{x_i}\\right) \\\\\n    &= \\sigma(w^c \\cdot bow(\\mathbf{x})) \n\\end{align*}$$\n\nWhy do we mention this? Because in your implementation, you can mkae use of either of these two approaches. You can convert the text into a sequence of word type indices or into a bag-of-words representation.\n\nThe calculation of $\\Prob(c \\given \\mathbf{x})$ for each text $\\mathbf{x}$ is referred to as the _forward_ computation. In summary, the forward computation for logistic regression involves a linear calculation ($\\sum_{i=1}^m w^c_{x_i}$ or $w^c \\cdot bow(\\mathbf{x})$) followed by a nonlinear calculation ($\\sigma$). We think of the perceptron (and more generally many of these neural network models) as transforming from one representation to another. A perceptron performs a linear transformation from the index or bag-of-words representation of the text to a representation as a vector followed by a nonlinear transformation, a sigmoid, giving a representation as a probability distribution over the class labels. This single-layer perceptron thus involves two _sublayers_. (In the next part of the problem set, you'll experiment with a multilayer perceptron, with two perceptron layers, and hence four sublayers.)\n\nThe loss function you'll use is the negative log probability $-\\log \\Prob (c \\given \\mathbf{x})$. The negative is used, since it is convention to minimize loss, whereas we want to maximize log likelihood. \n\nThe forward and loss computations are illustrated in the figure below. In practice, for numerical stability reasons, PyTorch absorbs the softmax operation into the loss function `nn.CrossEntropyLoss`. That is, the input to the `nn.CrossEntropyLoss` function is the vector of sums $\\sum_{i=1}^m w^c_{x_i}$ (labeled as \"your output\" in the figure) rather than the vector of probabilities $\\Prob(c \\given \\mathbf{x})$. That makes things easier for you (!), since you're responsible only for the first sublayer.\n\n<img src=\"https://raw.githubusercontent.com/nlp-course/data/master/img/logistic_regression.png\" alt=\"LR_illustration\" width=\"400\"/>\n\nGiven a forward computation, the weights can then be adjusted by taking a step opposite to the gradient of the loss function. Adjusting the weights in this way is referred to as the _backward_ computation. Fortunately, `torch` takes care of the backward computation for you.\n\nThe optimization process of performing the forward computation, calculating the loss, and performing the backward computation to improve the weights is done repeatedly until the process converges on a (hopefully) good set of weights. You'll find this optimization process in the `train_all` method that we've provided. The trained weights can then be used to perform classification on a test set. See the `evaluate` method.\n\nYou'll be responsible for implementing the forward computation as a method `forward`. We have provided code for performing the optimization and evaluation (though you should feel free to change them). "}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "KEgCwQVHrVw0"}, "source": "### Implement a logistic regression classifier\n\nFor the implementation, we ask you to implement a logistic regression classifier as a subclass of the [`torch.nn` module](https://pytorch.org/docs/stable/nn.html). You will be adding the following two methods:\n\n1. `__init__`: an initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples.\n\n    During initialization, you'll want to define a [tensor](https://pytorch.org/docs/stable/tensors.html#torch-tensor) of weights, [initialized randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_). This tensor is a [parameter](https://pytorch.org/docs/master/generated/torch.nn.parameter.Parameter.html#torch.nn.parameter.Parameter) of the `torch.nn` instance in the following special technical sense: It is the parameters of the module whose gradients will be calculated and whose values will be updated. Alternatively, you might find it easier to use the [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) which is a wrapper to the weight tensor with a lookup implementation.\n\n2. `forward`: given a text batch of size `batch_size X max_length`, return a tensor of logits of size `batch_size X num_labels`. That is, for each text $\\mathbf{x}$ in the batch and each label $c$, you'll be calculating $\\sum_{i=1}^m w_{x_i}^c$ as shown in the illustration above, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you won't need that.\n\nSome things to consider:\n\n1. The parameters of the model, the weights need to be initialized properly. We suggest initializing them to some small random values. See [`torch.uniform_`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_).\n\n2. You'll want to make sure that padding tokens are handled properly. What should the weight be for the padding token?\n\n3. In extracting the proper weights to sum up, based on the word types in a sentence, we are essentially doing a lookup operation. You might find [`nn.Embedding`](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) or [`torch.gather`](https://pytorch.org/docs/stable/generated/torch.gather.html#torch-gather) useful.\n\nYou should expect to achieve about **90%** accuracy on the ATIS classificiation task. "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "_KV-c1XX8726"}, "outputs": [], "source": "#TODO\nclass LogisticRegression(nn.Module):\n  def __init__ (self, text, label):\n    super().__init__()\n    self.text = text\n    self.label = label\n    self.padding_id = text.vocab.stoi[text.pad_token]\n    # Keep the vocabulary sizes available\n    self.N = len(label.vocab.itos) # num_classes\n    self.V = len(text.vocab.itos)  # vocab_size\n    # Specify cross-entropy loss for optimization\n    self.criterion = nn.CrossEntropyLoss()\n    # TODO: create and initialize a tensor for the weights,\n    #       or create an nn.Embedding module and initialize\n    raise NotImplementedError\n\n  def forward(self, text_batch):\n    # TODO: calculate the logits for the batch, \n    #       returning a tensor of size batch_size x num_labels\n    raise NotImplementedError    \n\n  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n    # Switch the module to training mode\n    self.train()\n    # Use Adam to optimize the parameters\n    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n    best_validation_accuracy = -float('inf')\n    best_model = None\n    # Run the optimization for multiple epochs\n    for epoch in range(epochs):\n      c_num = 0\n      total = 0\n      running_loss = 0.0\n      for batch in tqdm(train_iter):\n        # Zero the parameter gradients\n        optim.zero_grad()\n\n        # Input and target\n        text = batch.text           # a tensor of shape (bsz, max_len)\n        logits = self.forward(text) # perform the forward computation\n        target = batch.label.long() # bsz\n        batch_size = len(target)\n\n        # Compute the loss\n        loss = self.criterion(logits, target)\n\n        # Perform backpropagation\n        loss.backward()\n        optim.step()\n\n        # Prepare to compute the accuracy\n        predictions = torch.argmax(logits, dim=1)\n        total += batch_size\n        c_num += (predictions == target).float().sum().item()        \n        running_loss += loss.item() * batch_size\n\n      # Evaluate and track improvements on the validation dataset\n      validation_accuracy = self.evaluate(val_iter)\n      if validation_accuracy > best_validation_accuracy:\n        best_validation_accuracy = validation_accuracy\n        self.best_model = copy.deepcopy(self.state_dict())\n      epoch_loss = running_loss / total\n      epoch_acc = c_num / total\n      print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n             f'Training accuracy: {epoch_acc:.4f} '\n             f'Validation accuracy: {validation_accuracy:.4f}')\n\n  def evaluate(self, iterator):\n    self.eval()   # switch the module to evaluation mode\n    total = 0     # running total of example\n    c_num = 0     # running total of correctly classified examples\n    for batch in tqdm(iterator):\n      text = batch.text\n      logits = self.forward(text)                 # calculate forward probabilities\n      target = batch.label.long()                 # extract gold labels\n      predictions = torch.argmax(logits, dim=-1)  # calculate predicted labels\n      total += len(target)\n      c_num += (predictions == target).float().sum().item()\n    return c_num / total"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "6wW6UcFqv1zp"}, "outputs": [], "source": "# Instantiate classifier and run it\nmodel = LogisticRegression(TEXT, LABEL).to(device) \nmodel.train_all(train_iter, val_iter)\nmodel.load_state_dict(model.best_model)\ntest_accuracy = model.evaluate(test_iter)\nprint (f'Test accuracy: {test_accuracy:.4f}')"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "Te35cWGOJlf9"}, "source": "## Part 3: Multilayer perceptron\n\n### Review of multilayer perceptrons\n\nIn the last part, you implemented a perceptron, a model that involved a linear calculation (the sum of weights) followed by a nonlinear calculation (the softmax, which converts the summed weight values to probabilities). In a multi-layer perceptron, we take the output of the first perceptron to be the input of a second perceptron (and of course, we could continue on with a third or even more).\n\nIn this part, you'll implement the forward calculation of a two-layer perceptron, again letting PyTorch handle the backward calculation as well as the optimization of parameters. The first layer will involve a linear summation as before and a sigmoid as the nonlinear function. The second will involve a linear summation and a softmax (the latter absorbed, as before, into the loss function). Thus, the difference from the logistic regression implementation is simply the adding of the sigmoid and second linear calculations. See the figure for the structure of the computation. \n\n<img src=\"https://raw.githubusercontent.com/nlp-course/data/master/img/MLP.png\" alt=\"MLP_illustration\" width=\"400\"/>\n\n"}, {"cell_type": "markdown", "metadata": {"colab_type": "text", "id": "FsBnCFe0CnUv"}, "source": "### Implement a multilayer perceptron classifier\n\nFor the implementation, we ask you to implement a two layer perceptron classifier, again as a subclass of the [`torch.nn` module](https://pytorch.org/docs/stable/nn.html). You might reuse quite a lot of the code from logistic regression. As before, you will be adding the following two methods:\n\n1. `__init__`: An initializer that takes two `torchtext` fields providing descriptions of the text and label aspects of examples, and `hidden_size` specifying the size of the hidden layer (e.g., in the above illustration, `hidden_size` is 4).\n\n    During initialization, you'll want to define two tensors of weights, which serve as the parameters of this model, one for each layer. You'll want to [initialize them randomly](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.uniform_). \n    \n    The weights in the first layer are a kind of lookup (as in the previous part), mapping words to a vector of size `hidden_size`. The [`nn.Embedding` module](https://pytorch.org/docs/master/generated/torch.nn.Embedding.html) is a good way to set up and make use of this weight tensor.\n    \n    The weights in the second layer define a linear mapping from vectors of size `hidden_size` to vectors of size `num_labels`. The [`nn.Linear` module](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) or [`torch.mm`](https://pytorch.org/docs/master/generated/torch.mm.html) for matrix multiplication may be helpful here.\n\n2. `forward`: Given a text batch of size `batch_size X max_length`, the `forward` function returns a tensor of logits of size `batch_size X num_labels`. \n\n    That is, for each text $\\mathbf{x}$ in the batch and each label $c$, you'll be calculating $MLP(bow(\\mathbf{x}))$ as shown in the illustration above, returning a tensor of these values. Note that the softmax operation is absorbed into [`nn.CrossEntropyLoss`](https://pytorch.org/docs/master/generated/torch.nn.CrossEntropyLoss.html) so you don't need to worry about that.\n    \n    For the sigmoid sublayer, you might find [`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html) useful.\n\nYou should expect to achieve at least **90%** accuracy on the ATIS classificiation task. "}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "IdBVL9mKDMe_"}, "outputs": [], "source": "#TODO\nclass MultiLayerPerceptron(nn.Module):\n  def __init__ (self, label, text, hidden_size=128):\n    super().__init__ ()\n    self.text = text\n    self.label = label\n    self.padding_id = text.vocab.stoi[text.pad_token]\n    self.hidden_size = hidden_size\n    # Keep the vocabulary sizes available\n    self.N = len(label.vocab.itos) # num_classes\n    self.V = len(text.vocab.itos)  # vocab_size\n    # Specify cross-entropy loss for optimization\n    self.criterion = nn.CrossEntropyLoss()\n    # TODO: implement here\n    raise NotImplementedError\n\n  def forward(self, text_batch):\n    # TODO: implement here\n    raise NotImplementedError\n\n  def train_all(self, train_iter, val_iter, epochs=8, learning_rate=3e-3):\n    # Switch the module to training mode\n    self.train()\n    # Use Adam to optimize the parameters\n    optim = torch.optim.Adam(self.parameters(), lr=learning_rate)\n    best_validation_accuracy = -float('inf')\n    best_model = None\n    # Run the optimization for multiple epochs\n    for epoch in range(epochs):\n      c_num = 0\n      total = 0\n      running_loss = 0.0\n      for batch in tqdm(train_iter):\n        # Zero the parameter gradients\n        optim.zero_grad()\n\n        # Input and target\n        text = batch.text           # a tensor of shape (bsz, max_len)\n        logits = self.forward(text) # perform the forward computation\n        target = batch.label.long() # bsz\n        batch_size = len(target)\n\n        # Compute the loss\n        loss = self.criterion(logits, target)\n\n        # Perform backpropagation\n        loss.backward()\n        optim.step()\n\n        # Prepare to compute the accuracy\n        predictions = torch.argmax(logits, dim=1)\n        total += batch_size\n        c_num += (predictions == target).float().sum().item()        \n        running_loss += loss.item() * batch_size\n\n      # Evaluate and track improvements on the validation dataset\n      validation_accuracy = self.evaluate(val_iter)\n      if validation_accuracy > best_validation_accuracy:\n        best_validation_accuracy = validation_accuracy\n        self.best_model = copy.deepcopy(self.state_dict())\n      epoch_loss = running_loss / total\n      epoch_acc = c_num / total\n      print (f'Epoch: {epoch} Loss: {epoch_loss:.4f} '\n             f'Training accuracy: {epoch_acc:.4f} '\n             f'Validation accuracy: {validation_accuracy:.4f}')\n\n  def evaluate(self, iterator):\n    self.eval()   # switch the module to evaluation mode\n    total = 0     # running total of example\n    c_num = 0     # running total of correctly classified examples\n    for batch in tqdm(iterator):\n      text = batch.text\n      logits = self.forward(text)                 # calculate forward probabilities\n      target = batch.label.long()                 # extract gold labels\n      predictions = torch.argmax(logits, dim=-1)  # calculate predicted labels\n      total += len(target)\n      c_num += (predictions == target).float().sum().item()\n    return c_num / total"}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {}, "colab_type": "code", "id": "F4Bs0f7Hv1z4"}, "outputs": [], "source": "# Instantiate classifier and run it\nmodel = MultiLayerPerceptron(TEXT, LABEL).to(device) \nmodel.train_all(train_iter, val_iter)\nmodel.load_state_dict(model.best_model)\ntest_accuracy = model.evaluate(test_iter)\nprint (f'Test accuracy: {test_accuracy:.4f}')"}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "include_colab_link": true, "name": "project1_classification.ipynb", "provenance": [], "toc_visible": true}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.3"}}, "nbformat": 4, "nbformat_minor": 4}
